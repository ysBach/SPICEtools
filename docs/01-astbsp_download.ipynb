{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e31d87a",
   "metadata": {},
   "source": [
    "# BSP Download\n",
    "The purpose of this notebook is to\n",
    "1. Cell 1: Make a cURL script (\"a.txt\" and \"c.txt\" for asteroids and comets, respectively) to download millions of BSP files from JPL Horizons.\n",
    "    * This script is made based on SBDB query results.\n",
    "    * The query script (txt file) will be about 300 MB in total (!)\n",
    "2. Then the user should run the script on the terminal using `curl`\n",
    "    * This will try downloading millions of files, one by one, and will take ~1-2 weeks.\n",
    "3. Cell 1 (again): After doing that, you will realize some downloads were problematic. Run the cell (used in step #1) again.\n",
    "    * This will remove erroneous BSP files and generate a smaller shell script that can be used for downloading only those erroneous cases.\n",
    "4. Cell 2: The curl-downloaded files are in TXT format. We must convert them to BSP format.\n",
    "    * Run the cell, and it will do (1) convert TXT to BSP and save in a different directory && (2) zip the TXT for archive purposes (the user may delete these zip files or even skip zipping if they are not needed)\n",
    "    * Total ~ 5 hours on my laptop (MBP 14\" [2021, macOS 13.6.4, M1Pro(6P+2E/G16c/N16c/32G)])\n",
    "\n",
    "\n",
    "## Preparation\n",
    "\n",
    "The original suggestion, for the case of (65803) Didymos:\n",
    "\n",
    "``curl -s \"https://ssd.jpl.nasa.gov/api/horizons.api?format=text&COMMAND='65803%3B'&EPHEM_TYPE=SPK&START_TIME='2025-01-01'&STOP_TIME='2028-01-01'&OBJ_DATA=YES\" | awk '/REFGL1NQ/,0' | base64 --decode > a65803.bsp``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98187294-eded-443b-8cb5-f0fa7db24f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 0\n",
    "import base64\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def iterator(it):\n",
    "    try:\n",
    "        from tqdm import tqdm\n",
    "        return tqdm(it)\n",
    "    except ImportError:\n",
    "        return it\n",
    "\n",
    "\n",
    "scripts = Path(\"_query_scripts\")\n",
    "dir_bsp = dict(c=Path(\"spkbsp/c\"), a=Path(\"spkbsp/a\"))\n",
    "dir_txt = dict(c=Path(\"_spktxts/c\"), a=Path(\"_spktxts/a\"))\n",
    "scripts.mkdir(exist_ok=True)\n",
    "[dd.mkdir(exist_ok=True, parents=True) for dic in [dir_bsp, dir_txt] for dd in dic.values()]\n",
    "\n",
    "dtypes = {\n",
    "    \"spkid\": int, \"full_name\": str, \"soln_date\": str, \"condition_code\": str,\n",
    "}\n",
    "\n",
    "dfs = dict(\n",
    "    c=pd.read_parquet(\"sbdb_c_2024-08-02.parq\", columns=dtypes.keys()),\n",
    "    a=pd.read_parquet(\"sbdb_a_2024-08-02.parq\", columns=dtypes.keys()),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc16000",
   "metadata": {},
   "source": [
    "## Shell Script (cURL) for Downloading\n",
    "\n",
    "https://stackoverflow.com/questions/71244217/how-to-use-curl-z-parallel-effectively\n",
    "\n",
    "``{a/c}_all.txt`` are the shell scripts for all 1.3+M objects (just for recording+debugging... you may delete it), while ``{a/c}.txt`` are those for further downloads (e.g., when download stopped in the middle).\n",
    "\n",
    "Run the cell below whenever a new download is needed (or you want to be sure about broken files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "095e5348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c: 0 already downloaded\n",
      "You need to download all 3958 c spk files\n",
      "a: 0 already downloaded\n",
      "You need to download all 1386302 a spk files\n"
     ]
    }
   ],
   "source": [
    "# Cell 1\n",
    "def write_script(spkids, name, fpath, skip_existing=True):\n",
    "    if skip_existing and fpath.exists():\n",
    "        return\n",
    "    with open(fpath, \"w\") as ff:\n",
    "        for spkid in spkids:\n",
    "            _write_script(spkid, name, ff)\n",
    "\n",
    "\n",
    "def _write_script(spkid, name, filehandle):\n",
    "    suffix = \"%3BCAP\" if name.startswith(\"c\") else \"\"\n",
    "    filehandle.write(\n",
    "        \"url=https://ssd.jpl.nasa.gov/api/horizons.api?format=text&COMMAND='\"\n",
    "        + f\"DES={spkid}{suffix}%3B'&EPHEM_TYPE=SPK&\"\n",
    "        + \"START_TIME='2025-01-01'&STOP_TIME='2028-01-01'&OBJ_DATA=NO\\n\"\n",
    "    )\n",
    "    output = f\"./_spktxts/{name}/spk{spkid}.txt\"\n",
    "    filehandle.write(f\"output=\\\"{output}\\\"\\n\")\n",
    "\n",
    "\n",
    "def check_and_get_spkid(fpath):\n",
    "    if (size := fpath.stat().st_size) > 1000:  # > 1 kB\n",
    "        return int(fpath.stem[3:])\n",
    "    else:\n",
    "        fpath.unlink()\n",
    "        print(f\"Deleted {fpath} because its size is {size/1000:.1f} kB < 50 kB\")\n",
    "\n",
    "\n",
    "for name, folder in dir_bsp.items():\n",
    "    spkid_already_downloaded = {check_and_get_spkid(fpath) for fpath in folder.glob(\"spk*.[tzb][xis][tp]\")}\n",
    "\n",
    "    print(f\"{name}: {len(spkid_already_downloaded)} already downloaded\")\n",
    "\n",
    "    fpath_script_all = scripts/f\"{name}_all.txt\"\n",
    "    fpath_script_part = scripts/f\"{name}.txt\"\n",
    "    spkid_all = dfs[name][\"spkid\"]\n",
    "\n",
    "    if len(spkid_already_downloaded) == 0:  # Fresh download\n",
    "        print(f\"You need to download all {len(spkid_all)} {name} spk files\")\n",
    "        write_script(spkid_all, name, fpath_script_all, skip_existing=True)\n",
    "        from shutil import copy\n",
    "        copy(fpath_script_all, fpath_script_part)\n",
    "\n",
    "    elif len(spkid_already_downloaded) == len(spkid_all):  # No need to download...\n",
    "        print(f\"All {len(spkid_all)} {name} already downloaded\")\n",
    "        continue\n",
    "\n",
    "    else:  # len(spkid_already_downloaded) > 0\n",
    "        spkid2download = set(spkid_all) - spkid_already_downloaded\n",
    "        print(f\"You need to download {len(spkid2download)} {name} spk files\")\n",
    "        write_script(spkid_all, name, fpath_script_all, skip_existing=True)\n",
    "        write_script(spkid2download, name, fpath_script_part, skip_existing=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee98929",
   "metadata": {},
   "source": [
    "After the code above, when you have further ``spk`` files to download, run\n",
    "\n",
    "    curl --parallel --parallel-immediate --parallel-max 2 --config _query_scripts/c.txt\n",
    "    curl --parallel --parallel-immediate --parallel-max 2 --config _query_scripts/a.txt\n",
    "\n",
    "## Base64 decoding\n",
    "Below, base64 decoding, Zipping, and deleting took ~10ms/file (~ 300 min for all 1.3+M files) on the MBP 14\" [2021, macOS 13.1, M1Pro(6P+2E/G16c/N16c/32G)].\n",
    "\n",
    "* **TIP**: You can run the next cell multiple times while ``curl`` is still downloading. (I did not take the effort to make this code \"monitor\" the directory in real time...)\n",
    "* **TIP**: After all downloaded `.txt`'s are converted to `.bsp`, run the previous cell & `curl` on shell again to download erroneous files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "282c8535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with _spktxts/c/spk1003228.txt\n",
      "Error with _spktxts/c/spk1003776.txt\n",
      "Error with _spktxts/a/spk20101955.txt\n",
      "Error with _spktxts/a/spk20134340.txt\n"
     ]
    }
   ],
   "source": [
    "# Cell 2\n",
    "def decompress_and_delete(fpath, delete=True):\n",
    "    with zipfile.ZipFile(fpath, \"r\") as z:  # ~ 0.5ms / file\n",
    "        z.extractall(fpath.parent)\n",
    "    if delete:\n",
    "        fpath.unlink()\n",
    "\n",
    "\n",
    "def compress_and_delete(fpath, delete=True):  # << 1 ms to 10+ ms / file ???\n",
    "    with zipfile.ZipFile(f\"{fpath.parent}/{fpath.stem}.zip\", \"w\", zipfile.ZIP_DEFLATED, compresslevel=9) as z:\n",
    "        z.write(fpath, arcname=f\"{fpath.stem}.txt\")\n",
    "    if delete:\n",
    "        fpath.unlink()\n",
    "\n",
    "\n",
    "def save_b64decode(fpath, output, compress=True, delete=True):\n",
    "    with open(fpath, \"r\") as ff:\n",
    "        txt = ff.read()\n",
    "\n",
    "    try:\n",
    "        with open(output, \"wb\") as ff:\n",
    "            ff.write(base64.b64decode(\"REFGL1NQ\" + txt.split(\"REFGL1NQ\", 1)[1]))\n",
    "        if compress:\n",
    "            compress_and_delete(fpath, delete=delete)\n",
    "    except:  # No name for the error.. it is just ``Error: Incorrect padding``\n",
    "        print(f\"Error with {fpath}\")\n",
    "        output.unlink()\n",
    "\n",
    "\n",
    "for name, folder in dir_txt.items():\n",
    "    # for fpath in folder.glob(\"*.zip\"):\n",
    "    #     try:\n",
    "    #         decompress_and_delete(fpath, delete=True)\n",
    "    #     except zipfile.BadZipFile:\n",
    "    #         continue\n",
    "    for fpath in folder.glob(\"*.txt\"):\n",
    "        save_b64decode(fpath, dir_bsp[name]/f\"{fpath.stem}.bsp\", compress=True, delete=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
